## 4.5 Lichtenstein{.tabset}

```{r, warning=FALSE, echo=FALSE, results='hide'}
# Investigating Lichtenstein closely before 2008
Lichtenstein_before_2008 <- data_elec %>% select(Date,Lichtenstein) %>% filter(Date <= as.Date("2008-01-01"))

# Investigating Lichtenstein closely after 2008
Lichtenstein_after_2008 <- data_elec %>% select(Date,Lichtenstein) %>% filter(Date >= as.Date("2008-01-01"))

#creating a time period from 2010-2016 for lichtenstein -> this period will be used for training.
Lichtenstein_2010_2016 <- data_elec %>% select(Date,Lichtenstein) %>% filter(Date >= as.Date("2010-01-01") & Date <= as.Date("2015-12-31"))

#differencing the 2010-2016 data
#Lichtenstein_2010_2016 <- Lichtenstein_2010_2016 %>% select(Lichtenstein) %>% mutate(Lichtenstein = difference(Lichtenstein))

#taking the log of the values of lichtenstein -> careful because log(0) = -infinity
Lichtenstein_2010_2016$Lichtenstein <- log(Lichtenstein_2010_2016$Lichtenstein)

# Change "-Inf" to 0 in the "Lichtenstein" column
Lichtenstein_2010_2016$Lichtenstein[Lichtenstein_2010_2016$Lichtenstein == -Inf] <- 0





#just log of all data for licht
Lichtenstein_log <- data_elec %>% select(Date,Lichtenstein)
#taking the log of the values of lichtenstein -> careful because log(0) = -infinity
Lichtenstein_log$Lichtenstein <- log(Lichtenstein_log$Lichtenstein)

# Change "-Inf" to 0 in the "Lichtenstein" column
Lichtenstein_log$Lichtenstein[Lichtenstein_log$Lichtenstein == -Inf] <- 0

#This will be our new test set
logfrom2016 <- Lichtenstein_log %>% filter(Date >= as.Date("2016-01-01"))


```


### **Testing Models**
Below is the plot of how well each model fits the data based on a 60-period prediction starting from 2016. This code fits different models to the training data, generates forecasts, and then applies a constraint to the forecasted mean values to ensure they are not negative.
Based on the previous analysis Lichtenstein data is NON stationary time series, so we had to difference or log the data to make it stationary before using our models. We tried both, and it seemed like using the log of the data worked best.

Another thing to mention about Lichtenstein, is that for the ETS, ARIMA and SNaive models, we decided to train them only after 2010. We made this decision because before 2010, the data was very different (upward trending at a much higher level) and thought that if the models looked at that period, they would probably not produce accurate results.

```{r, warning=FALSE, echo=FALSE}
#Split the data
training_Licht <- data_elec %>% select(Date, Lichtenstein) %>%  filter(Date < as.Date("2016-01-01"))
test_Licht <- data_elec %>% select(Date, Lichtenstein) %>% filter(Date >= as.Date("2016-01-01"))


# Fit ARIMA model and generate forecasts
data_fit_Licht_a <- Lichtenstein_2010_2016 %>% #the training for arima is the logged values from 2010-2016
  model(ARIMA(Lichtenstein))
forecasts_Licht_a <- data_fit_Licht_a %>%
  forecast(h = 60)


# Fit ETS model and generate forecasts
data_fit_Licht_e <- Lichtenstein_2010_2016 %>% #the training for ets is the logged values from 2010-2016
  model(ETS(Lichtenstein))
forecasts_Licht_e <- data_fit_Licht_e %>%
  forecast(h = 60)
forecasts_Licht_e$.mean <- ifelse(forecasts_Licht_e$.mean < 0, 0, forecasts_Licht_e$.mean)

# Fit seasonal naive model and generate forecasts
data_fit_Licht_n <- Lichtenstein_2010_2016 %>% #the training for SNAIVE is the logged values from 2010-2016
  model(SNAIVE(Lichtenstein))
forecasts_Licht_n <- data_fit_Licht_n %>%
  forecast(h = 60)
forecasts_Licht_n$.mean <- ifelse(forecasts_Licht_n$.mean < 0, 0, forecasts_Licht_n$.mean)


# Combine forecasts into a single data frame
all_forecasts_Licht <- bind_rows(
  data.frame(Date = forecasts_Licht_a$Date, Method = "ARIMA", Forecast = forecasts_Licht_a$.mean),
  data.frame(Date = forecasts_Licht_e$Date, Method = "ETS", Forecast = forecasts_Licht_e$.mean),
  data.frame(Date = forecasts_Licht_n$Date, Method = "SNAIVE", Forecast = forecasts_Licht_n$.mean))

# Plot all three sets of forecasts together
Lichtenstein_log %>% autoplot(Lichtenstein) +
  geom_line(data = all_forecasts_Licht, aes(x = Date, y = Forecast, color = Method)) +
  labs(title = "Electricity Demand in Lichtenstein")

```

### **Measuring Model Accuracy**
In our case S-Naive outperform during the model selection and analysis of accuracy. However S-Naive method may perform well for the time period that we have train our data, but it is important to note that it has some limitations. Indeed, it assumes that historical patterns will repeat exactly in the future, which may not always hold. The S-Naive method may not account for underlying factors or trends that could impact future outcomes. Therefore, there is an increased risk of inaccurate forecasts if the method is blindly applied without considering other relevant factors.
ETS could be great models based on a trade off between forecasting error and complexity of the model. ETS model has the third lowest RMSE and second lowest MAE which makes him a good candidate. In order to select a model, we can analyse residuals of those models

```{r, warning=FALSE, echo=FALSE}
#Accuracy
Licht_accuracy_a <- accuracy(forecasts_Licht_a, logfrom2016)

Licht_accuracy_e <-accuracy(forecasts_Licht_e, logfrom2016)

Licht_accuracy_n <-accuracy(forecasts_Licht_n, logfrom2016)

#Licht_accuracy_pw_a <- accuracy(forecasts_Licht_pw_a, logfrom2016)

Licht_accuracy_table <- rbind(
  data.frame(Licht_accuracy_a),
  data.frame(Licht_accuracy_e),
  data.frame(Licht_accuracy_n))
#AIC
#report(data_fit_Licht_a)
#report(data_fit_Licht_pw_a)
```

```{r, warning=FALSE, echo=FALSE}
Licht_accuracy_table[ 1:3,1:10] %>%
  kbl() %>%
  kable_paper(full_width = F) %>%
  column_spec(4, color = "white",
              background = spec_color(Licht_accuracy_table$RMSE[1:4],end = 0.6)) %>%
    column_spec(5, color = "white",
              background = spec_color(Licht_accuracy_table$MAE[1:4],end = 0.6))
```
```{r}
#report(data_fit_Licht_e)
#report(data_fit_Licht_n)
```

### **Residuals**
It is interesting to examine any outliers or extreme values in the residuals. The ACF analysis for SNaive shows Positive correlations between the ts and its lags. In opposite ETS model shows that there's no correlation between thoses values. Thus, each observation in the ts can be considered unrelated to the previous values. Consequently, Lichtenstein ts might need covariate to forecast and analyse it adequately. Concerning the residual, it should ideally appear random, in our case residuals are chinked around zero for both models (SNaive & ETS), thus those might shows autocorrelation.

```{r, warning=FALSE, echo=FALSE}
#gg_tsresiduals(data_fit_Licht_a) + labs(title = "ARIMA Lichtenstein residuals analysis")
gg_tsresiduals(data_fit_Licht_n) + labs(title = "SNAIVE Lichtenstein residuals analysis")
#gg_tsresiduals(data_fit_Licht_e) + labs(title = "ETS Lichtenstein residuals analysis")
#gg_tsresiduals(data_fit_Licht_pw_a) + labs(title = "PIECE WISE Lichtenstein residuals analysis")
```
