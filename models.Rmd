# Models

# Questions

- Naive model forecast is not showing me historical data, only the forecast
- Should we even bother doing a SES model (Simple exponential smoothing)? It's basically the same as the naive and don't think adds any value

# Building models - Only looking at Total exports for now

In the world of forecasting, we can't get a perfect forecast no matter how hard we try. Also, making a model more complicated won't necessarily make it more accurate. Finally, when trying to forecast it's important to take into consideration how much time is spent on a forecast, versus how much valuable information we get out of it. It is therefore important to consider simpler, more "naive" models to try and make predictions about the future, before moving on to more complicated ones. 

## Naive model 

The naive model we are going to use is simply a forecast that is equal to the latest observation. 

```{r}
#Creating and Plotting the naive model

elec_fit <- data_elec %>% model(NAIVE(Total))
augment(elec_fit) %>% ggplot(aes(x = Date)) +
  geom_line(aes(y = Total, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  scale_color_manual(values = c(Data = "blue", Fitted = "orange")
  ) + guides(colour = guide_legend(title = "Series"))

```

This plot is basically shows the original data, and then a copy of it which is moved one period in the future. 

Now let's look at the residuals of the naive forecasting model, and search for autocorrelation. It is important to do so, because if there is autocorrelation in a the residuals of a forecasting model, this means that the model is not capturing some of the important features of the data, and might be leading to an inaccurate forecast. We will use the Portmanteau (Ljung-box) test:

This test checks for the presence of autocorrelation in the residuals, by testing the following null hypothesis: $H_0:$ The residuals are independently and identically distributed (IID) with a normal distribution. If the p-value of the test is less than the chosen significance level (example $\alpha= 0.05$), then we reject the null hypothesis and conclude that there is evidence of autocorrelation in the residuals.

```{r}
#plotting the residuals of the naive method
aug <- elec_fit %>% augment()
autoplot(aug, .resid) +
  labs(x = "Month", y = "Residual", title = "Residuals from naÃ¯ve method")

#Box-pierce test
aug %>% features(.resid, box_pierce, lag = 10, dof = 0) #The degrees of freedom (dof) are 0 for the naive method

#Ljung test
aug %>% features(.resid, ljung_box, lag = 10, dof = 0)

```

From both the Box-pierce and Ljung tests, we can see that the p-value is very small (very close to 0) which indicates the presence of autocorrelation in the residuals. This means that the residuals are not white noise, which is not good and we will have to work around it.

### Forecast for the next 6 months using the Naive model: 

Now that we have built the naive model, and explained that it might be inaccurate due to the presence of autocorrelation, let's create a forecast using it: 

```{r}

elec_fc <- fabletools::forecast(elec_fit, h = 6)

autoplot(elec_fc) + 
  xlab("Date") + 
  ylab("Electricity Exports") + 
  ggtitle("Naive Forecast for 6 months - Total Electricity Exports")

# Convert the forecast object to a tibble to be able to use it with the kable function
forecast_tbl <- elec_fc %>% as_tibble() %>% 
  select(Date, .mean)

# Use the kable function to display the table
kable(forecast_tbl, format = "markdown", 
      col.names = c("Date", "Forecast"), 
      align = "cccc")
```

As expected, the output of the Naive forecast is a flat line. This is because in this model, each forecast is equal to the value in the previous period. This isn't very useful and accurate, so we have to use more complicated and sophisticated models for the rest of this analysis.

## ETS model - Exponential Smoothing

ETS stands for Error-Trend-Seasonality, and is one of the most robust and commonly used forecasting methods in the industry. 

ETS is based on the idea that more recent observations should be given more importance when creating a forecast. It's a method that allocates weights to each observation, and the weights are bigger, the more recent the data is. As we go back in time, the weights become exponentially smaller, giving less and less weight/importance on older data.

In our case, the most recent data (2020-2022) were probably heavily impacted by Covid-19 and the war between Russia and Ukraine. This means that an ETS model, will put more weight on more recent data, hence will give us a biased forecast based on those two big events. We have to be careful when selecting the weights of the ETS model, so that not too much weight is put on the most recent data. 


### Simple Exponential Smoothing 

This version of Exponential smoothing, assumes that the data has no Trend and no Seasonality:

```{r}


```

### Exponential smoothing with different alpha parameters

The classical exponential smoothing function, will require a variable alpha ($\alpha$) which determines the weight on different observations over time. The higher the value of alpha, the more weight is given on more recent observations. Let's look at the forecast of an ETS model which gives a lot of weight on recent observations, and much less on those further back in time.

```{r}

fit <- algeria_economy %>%
  model(ETS(Exports ~ error("A") + trend("N", alpha = 0.1) + season("N")))
fit %>% forecast(h = 12) %>% autoplot(algeria_economy, level = 0) +
theme(legend.position = "none")

```





